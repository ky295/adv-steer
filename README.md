# SPAR Project 

## Related materials 
**Exploitation of linear representation hypothesis**
- (Zou et al., 2023a) Representation Engineering: A top-down approach to AI transparency
- (Zou et al., 2023b) Universal and Transferable Adversarial Attacks  on Aligned Language Models
- (Arditi et al., 2024) Refusal in Language Models Is Mediated by a Single Direction
- (Huang et al., 2024) Stronger Universal and Transfer Attacks by Suppressing Refusals
- (Lin et al., 2024) Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis
- (Turner et al., 2024) Steering Language Models with Activation Engineering
- (Thompson et al., 2024a) Fluent Dreaming for Language Models
- (Thompson et al., 2024b) FLRT: Fluent Student Teacher Redteaming

**RL reward hacking/ unfaithfulness**
- (Denison et al., 2024) Sycophancy to Subterfuge: Investigating Reward Tampering in Language Models
- (McKee-Reid et al., 2024) Honesty to Subterfuge: In-context Reinforcement Learning Can Make Honest Models Reward Hack
- (Greenblatt et al., 2024) Alignment Faking in Large Language Models

**Chain-of-thought reasoning**
- (Wei et al., 2023) Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
- (Yeo et al., 2025) Demystifying Long Chain-of-Thought Reasoning in LLMs
- (DeepSeek-AI et al., 2025) DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
