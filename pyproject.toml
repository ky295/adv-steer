[project]
name = "reasoning-manipulation"
version = "0.1.0"
description = "Adversarial Manipulation of Reasoning Models using Internal Representations"
authors = [
  {name = "Kureha Yamaguchi", email = "ky295@cantab.ac.uk"},
  {name = "Benjamin Etheridge"},
  {name = "Andy Arditi"},
]
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "datasets>=3.6.0",
    "einops>=0.8.1",
    "ipykernel>=6.29.5",
    "nnsight>=0.4.7",
    "numpy>=2.3.1",
    "pandas>=2.3.0",
    "protobuf>=6.31.1",
    "scikit-learn>=1.7.0",
    "scipy>=1.7.0",
    "seaborn>=0.13.2",
    "sentencepiece>=0.2.0",
    "strong-reject",
    "torch>=2.7.1",
    "transformers>=4.52.4",
    "wandb>=0.20.1",
]

[dependency-groups]
dev = [
    "isort>=6.0.1",
    "ruff>=0.12.0",
    "ty>=0.0.1a11",
]

[tool.uv.sources]
strong-reject = { git = "https://github.com/dsbowen/strong_reject.git", rev = "main" }

[project.scripts]
create_alpaca = "utils.dataset_alpaca:main"
create_strong_reject = "utils.dataset_strong_reject:main"
activations = "probing.activations:main"
orthogonalize = "probing.create_ortho_model:main"
ortho_csv_generation = "probing.ortho_csv_generation:main"
attack_experiments = "attack.run_experiments:main"
