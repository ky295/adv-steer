{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate responses using the StrongREJECT package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /workspace/adv-steer/venv/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: peft in /workspace/adv-steer/venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in /workspace/adv-steer/venv/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from bitsandbytes) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: transformers in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (1.4.0)\n",
      "Requirement already satisfied: safetensors in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/adv-steer/venv/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U bitsandbytes peft datasets\n",
    "# !pip install git+https://github.com/dsbowen/strong_reject@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 14 11:28:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:D2:00.0 Off |                    0 |\n",
      "|  0%   28C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required: Prepare the dataset\n",
    "\n",
    "First, we need a dataset with two columns:\n",
    "\n",
    "* `forbidden_prompt` is a harmful prompt that a model should be forbidden from answering\n",
    "* `response` is a model's response to the forbidden prompt\n",
    "\n",
    "We start with a simple script to transform a CSV file with \"prompt\" and \"response\" columns into a Hugging Face Dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 313it [00:00, 6566.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 313 rows\n",
      "Discarded rows (no </think> tag): 0\n",
      "Processed datasets: 313 rows\n",
      "\n",
      "Chain of Thought Dataset:\n",
      "Dataset({\n",
      "    features: ['forbidden_prompt', 'response'],\n",
      "    num_rows: 313\n",
      "})\n",
      "\n",
      "Output Dataset:\n",
      "Dataset({\n",
      "    features: ['forbidden_prompt', 'response'],\n",
      "    num_rows: 313\n",
      "})\n",
      "\n",
      "Dataset:\n",
      "Dataset({\n",
      "    features: ['forbidden_prompt', 'response'],\n",
      "    num_rows: 313\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read CSV file\n",
    "prompts = []\n",
    "responses = []\n",
    "cot_responses = []  # Chain of thought responses\n",
    "output_responses = []  # Final outputs\n",
    "discarded_count = 0  # Count discarded rows\n",
    "\n",
    "csv_path = \"../dataset/strongreject_reasoning_template.csv\"\n",
    "with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in tqdm(reader, desc=\"Processing rows\"):\n",
    "        \n",
    "        # Check if response contains the </think> tag\n",
    "        if \"</think>\" in row[\"response\"]:\n",
    "            prompts.append(row[\"prompt\"])\n",
    "            responses.append(row['response'])\n",
    "\n",
    "            # Split response at </think> tag\n",
    "            parts = row[\"response\"].split(\"</think>\", 1)  # Split at first occurrence\n",
    "\n",
    "            # Add chain of thought (everything before </think>)\n",
    "            cot_responses.append(parts[0].strip())\n",
    "            # Add output (everything after </think>)\n",
    "            output_responses.append(parts[1].strip())\n",
    "        else:\n",
    "            discarded_count += 1\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"forbidden_prompt\": prompts,\n",
    "    \"response\": responses\n",
    "})\n",
    "\n",
    "# Create chain of thought dataset\n",
    "cot_dataset = Dataset.from_dict({\n",
    "    \"forbidden_prompt\": prompts,\n",
    "    \"response\": cot_responses\n",
    "})\n",
    "\n",
    "# Create output dataset\n",
    "output_dataset = Dataset.from_dict({\n",
    "    \"forbidden_prompt\": prompts,\n",
    "    \"response\": output_responses\n",
    "})\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Original dataset: {len(prompts) + discarded_count} rows\")\n",
    "print(f\"Discarded rows (no </think> tag): {discarded_count}\")\n",
    "print(f\"Processed datasets: {len(prompts)} rows\")\n",
    "print(\"\\nChain of Thought Dataset:\")\n",
    "print(cot_dataset)\n",
    "print(\"\\nOutput Dataset:\")\n",
    "print(output_dataset)\n",
    "print(\"\\nDataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "872aa58ddad449589d4a973bc0e5da56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98609b9ffd82428b82c418a904106832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede9578ae9174db1859548dd6968f9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea96e9e7b31412c9abf460796e8f20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73323fe2e3c24d49b854337f3874435f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661e7a7dd07142c79d3fb08a05ddde2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcea259b480a411292d00acbae02a468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697fe62a2c944ddeb1dddcb06a74bdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e1cf6eb5304fc68d1422579c5b57c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35beb44634d942cdab167bf2d1caee52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/39.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656f64e4769c40a1a5b4d03c6cd9db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e113a84aa54deb9e790f7d645df70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca716449ed24b4ebdb8e900760fd01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7a183fbaad4c70be9389652da61e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['forbidden_prompt', 'response', 'score', 'evaluator'],\n",
      "    num_rows: 313\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from strong_reject.evaluate import evaluate_dataset\n",
    "\n",
    "eval_dataset = evaluate_dataset(output_dataset, [\"strongreject_finetuned\"])\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering based on the whole score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# def filter_csv(eval_dataset, cot_dataset, lower_threshold=None, upper_threshold=None):\n",
    "#     \"\"\"\n",
    "#     Filter rows with score < lower_threshold OR score > upper_threshold and add to filtered_data.\n",
    "    \n",
    "#     Args:\n",
    "#         dataset: A Hugging Face dataset object\n",
    "#         lower_threshold: Score should be below this value (if not None)\n",
    "#         upper_threshold: Score should be above this value (if not None)\n",
    "#     \"\"\"\n",
    "#     model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "#     if lower_threshold is None and upper_threshold is None:\n",
    "#         raise ValueError(\"At least one threshold (lower or upper) must be specified\")\n",
    "    \n",
    "#     # Filter rows based on provided thresholds\n",
    "#     filtered_data = []\n",
    "#     token_lengths = []\n",
    "#     for i, row in enumerate(eval_dataset):\n",
    "#         # Check if score is below lower_threshold OR above upper_threshold\n",
    "#         include_row = False\n",
    "        \n",
    "#         if lower_threshold is not None and row[\"score\"] < lower_threshold:\n",
    "#             include_row = True\n",
    "        \n",
    "#         if upper_threshold is not None and row[\"score\"] > upper_threshold:\n",
    "#             include_row = True\n",
    "            \n",
    "#         if include_row:\n",
    "\n",
    "#             filtered_data.append({\"forbidden_prompt\": row[\"forbidden_prompt\"], \"response\": row[\"response\"]})\n",
    "#             tokens = tokenizer.encode(cot_dataset[\"response\"][i])\n",
    "#             token_lengths.append(len(tokens))\n",
    "    \n",
    "#     smallest_token_length = min(token_lengths)\n",
    "    \n",
    "#     return filtered_data, smallest_token_length\n",
    "\n",
    "# def write_to_csv(filtered_data, output_file=None):\n",
    "\n",
    "#     # Write to CSV\n",
    "#     with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#         fieldnames = ['forbidden_prompt', 'response']\n",
    "#         writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "#         writer.writeheader()\n",
    "#         for row in filtered_data:\n",
    "#             writer.writerow(row)\n",
    "    \n",
    "#     print(f\"Successfully exported {len(filtered_data)} rows to {output_file}\")\n",
    "\n",
    "# # Uncomment and modify the example that fits your needs\n",
    "# filtered_data, smallest_token_length = filter_csv(eval_dataset, cot_dataset, lower_threshold=0.1)\n",
    "# # 106 the size of the non-cautious dataset\n",
    "# write_to_csv(filtered_data[:106], output_file=\"../dataset/filtered_low_strongreject.csv\")\n",
    "# print(\"\\nfiltered_low_advbench.csv\")\n",
    "# print(smallest_token_length)\n",
    "\n",
    "# # filtered_data, smallest_token_length = filter_csv(eval_dataset, cot_dataset, upper_threshold=0.85)\n",
    "# # write_to_csv(filtered_data, output_file=\"../dataset/filtered_high_advbench.csv\")\n",
    "# # print(\"\\nfiltered_high_advbench.csv\")\n",
    "# # print(smallest_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtering based only on the output score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully exported 116 rows to ../dataset/cautious_eval.csv\n",
      "\n",
      "cautious_eval.csv\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def filter_csv(eval_dataset, dataset, cot_dataset, lower_threshold=None, upper_threshold=None):\n",
    "    \"\"\"\n",
    "    Filter rows with score < lower_threshold OR score > upper_threshold and add to filtered_data.\n",
    "    \n",
    "    Args:\n",
    "        dataset: A Hugging Face dataset object\n",
    "        lower_threshold: Score should be below this value (if not None)\n",
    "        upper_threshold: Score should be above this value (if not None)\n",
    "    \"\"\"\n",
    "    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if lower_threshold is None and upper_threshold is None:\n",
    "        raise ValueError(\"At least one threshold (lower or upper) must be specified\")\n",
    "    \n",
    "    # Filter rows based on provided thresholds\n",
    "    filtered_data = []\n",
    "    token_lengths = []\n",
    "    for i, row in enumerate(eval_dataset):\n",
    "        # Check if score is below lower_threshold OR above upper_threshold\n",
    "        include_row = False\n",
    "        \n",
    "        if lower_threshold is not None and row[\"score\"] < lower_threshold:\n",
    "            include_row = True\n",
    "        \n",
    "        if upper_threshold is not None and row[\"score\"] > upper_threshold:\n",
    "            include_row = True\n",
    "            \n",
    "        if include_row:\n",
    "\n",
    "            filtered_data.append({\"forbidden_prompt\": row[\"forbidden_prompt\"], \"response\": dataset[\"response\"][i]})\n",
    "            tokens = tokenizer.encode(cot_dataset[\"response\"][i])\n",
    "            token_lengths.append(len(tokens))\n",
    "    \n",
    "    smallest_token_length = min(token_lengths)\n",
    "    \n",
    "    # Check if we have any matching rows\n",
    "    if not filtered_data:\n",
    "        print(\"No rows found matching the specified criteria\")\n",
    "        return\n",
    "    \n",
    "    return filtered_data, smallest_token_length\n",
    "\n",
    "def write_to_csv(filtered_data, output_file=None):\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['forbidden_prompt', 'response']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for row in filtered_data:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Successfully exported {len(filtered_data)} rows to {output_file}\")\n",
    "\n",
    "# Uncomment and modify the example that fits your needs\n",
    "filtered_data, smallest_token_length = filter_csv(eval_dataset, dataset, cot_dataset, lower_threshold=0.03)\n",
    "# 106 the size of the non-cautious dataset\n",
    "write_to_csv(filtered_data, output_file=\"../dataset/cautious_eval.csv\")\n",
    "print(\"\\ncautious_eval.csv\")\n",
    "print(smallest_token_length)\n",
    "\n",
    "# filtered_data, smallest_token_length = filter_csv(eval_dataset, dataset, cot_dataset, upper_threshold=0.85)\n",
    "# write_to_csv(filtered_data, output_file=\"../dataset/filtered_high_advbench.csv\")\n",
    "# print(\"\\nfiltered_high_advbench.csv\")\n",
    "# print(smallest_token_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
